Relus are a common activation function used to give a nonlinear relationship to the data after it passes through a neural layer.
![[Pasted image 20230801162110.png]]


### Code
```
tf.keras.Dense(units=, activation=tf.nn.relu)
```
