These are used often for the purpose of splitting up large training networks so we aren't bombarding our memory with a bunch of data at once. The weight update that we do every time we complete a min-batch is also beneficial in progressively changing our model.

The important differences of each of the batch approaches: [Differences Btw Epoch, Batch, and Mini-batch](https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batch#:~:text=So%2C%20a%20batch%20is%20equal,in%20mini%2Dbatch%20gradient%20descent.)

![[Pasted image 20230802201350.png]]